\Opensolutionfile{solution_file}[sols_chap_09]
% в квадратных скобках фактическое имя файла

\section{Условное ожидание}

Понятие «условное ожидание» используется в двух смыслах:
\begin{itemize}
\item $\E(Y|A)$, где $Y$ - случайная величина и $A$ - событие. В этом случае $\E(Y|A)$ - число.
\item $\E(Y|\mathcal{H})$, где $Y$ - случайная величина и $\mathcal{H}$ - \s-алгебра. В этом случае $\E(Y|\mathcal{H})$ - случайная величина.
\end{itemize}

\subsection{Условное ожидание как константа}

Условное ожидание $\E(Y|A)$ скорее всего знакомо человеку, освоившему вводный курс теории вероятностей. Условное ожидание $\E(Y|A)$ считается точно так же, как и обычное $\E(Y)$, только вместо обычной вероятности $\P(\cdot)$ используется условная вероятность $\P(\cdot|A)$. Если $A$ - событие с положительной вероятностью, то мы можем сменить вероятность каждого события $C$ с $\P(C)$ на $\tilde{P}(C):=\P(C|A)$. В упр. (\ldots ) мы видели, что $\tilde{P}$ - это действительно вероятность на \s-алгебре \F. Значит можно считать математическое ожидание (интегрировать) относительно нее:
\begin{mydef}
\label{def:alter_p}
Условное ожидание величины $Y$ относительно события $A$, с $\P(A)>0$ определяется по формуле:
\begin{equation}
\E(Y|A):=E_{\tilde{P}}(Y)=\int_{\Omega}Yd\tilde{P},
\end{equation}
где $\tilde{P}(\cdot):=\P(\cdot|A)$.
\end{mydef}

Можно дать и эквивалентное определение:

\begin{mydef}
\label{def:with_indic}
Условное ожидание величины $Y$ относительно события $A$, с $\P(A)>0$ определяется по формуле:
\begin{equation}
\E(Y|A):=\frac{\E(1_{A}Y)}{\P(A)}
\end{equation}
\end{mydef}

\begin{myth}
Определения \ref{def:alter_p} и \ref{def:with_indic} эквивалентны.
\end{myth}
\begin{proof}
Cтандартная пошаговая процедура.

Для простых случайных величин эти определения эквивалентны:
\begin{multline}
\int_{\Omega}Yd\tilde{P}=\sum_{i=1}^{n}Y_{i}\P(Y=y_{i}|A)=\sum_{i=1}^{n}y_{i}\frac{\P(Y=y_{i}\cap A)}{\P(A)} \\
=\frac{1}{\P(A)}\sum_{i=1}^{n}y_{i}\E(1_{A}1_{Y=y_{i}})=\frac{1}{\P(A)}E\left(\sum y_{i}1_{Y=y_{i}1_{A}}\right)
=\frac{\E(1_{A}Y)}{\P(A)}
\end{multline}
А дальше применяется MCT\ldots
\end{proof}

\subsection{Условное ожидание как случайная величина. Конечный случай.}

Гораздо более интересно условное ожидание относительно \s-алгебры!

Для начала рассмотрим конечно-порожденную \s-алгебру.

Пример. (\ldots )

\begin{mydef}
Если $A_{1}$, \ldots , $A_{n}$ попарно не пересекаются, $\P(A_{i})>0$ и $\mathcal{H}=\s(\{A_{i}\})$, то
\begin{equation}
\label{e_cond_simple}
\E(Y|\mathcal{H}):=\sum_{i}\E(Y|A_{i})1_{A_{i}}
\end{equation}
\end{mydef}

По этому определению, если произойдет $A_{1}$, то величина $\E(Y|\mathcal{H})$ будет равна $\E(Y|A_{1}$), если произойдет $A_{2}$, то величина $\E(Y|\mathcal{H})$ будет равна $\E(Y|A_{2}$) и т.д. А поскольку мы заранее не знаем, какое из $A_{i}$ произойдет, то $\E(Y|\mathcal{H})$ является случайной величиной.

Для краткости обозначим $\hat{Y}:=\E(Y|\mathcal{H})$.

Как правильно думать об условном математическом ожидании $\E(Y|\mathcal{H})$?
\begin{itemize}
\item Во-первых, полезно представлять себе $\E(Y|\mathcal{H})$ как «грубую», усредненную, версию $Y$. На множестве $A_{i}$ случайная величина $Y$ может принимать разные значения. А случайная величина $\E(Y|\mathcal{H})$ на множестве $A_{i}$ принимает фиксированное значение $\E(Y|A_{i})$, а это не что иное, как среднее значение величины $Y$ на множестве $A_{i}$.

(картинка)

\item Проекция $Y$ на множество $\mathcal{H}$-измеримых случайных величин. Если рассмотреть все $\mathcal{H}$-измеримые случайные величины, то $\E(Y|\mathcal{H})$ --- самая похожая на случайную величину $Y$. «Самая похожая» в том смысле, что расстояние $||Y-\E(Y|\mathcal{H})||_{L^{2}}$ - наименьшее из возможных.


(рисунок)
\item $ \hat{Y} $ --- как решение системы уравнений. С помощью ковариаций и мат. ожидания нельзя почувствовать разницу между $Y$ и $\hat{Y}$! А именно, наш прогноз $\hat{Y}$ - это единственное решение системы уравнений:

\begin{equation}
\begin{cases}
\E(\hat{Y})=\E(Y) \\
Cov(X_{1},\hat{Y})=Cov(X_{1},Y) \\
Cov(X_{2},\hat{Y})=Cov(X_{2},Y) \\
\ldots  \\
Cov(X_{n},\hat{Y})=Cov(X_{n},Y) \\
\end{cases}
\end{equation}
где $X_{i}$ - это всевозможные\footnote{Здесь есть некоторая вольность: таких случайных величин бесконечное количество, а вовсе не $n$. Но в случае конечно порожденной $ \sigma $-алгебры можно ограничиться индикаторами $ A_{i} $}   $\mathcal{H}$-измеримые случайные величины.

\end{itemize}

\begin{myex}
здесь дискретный пример с иллюстрацией трех свойств.

\end{myex}

Доказательство корректности второй и третьей интерпретации мы проведем для произвольной $\mathcal{H}$. Но для начала надо обобщить само определение условного математического ожидания на случай произвольной \s-алгебры. Каждую из трех идей можно довести до определения подходящего в общем случае. Кратко о каждом способе:

\begin{itemize}

\item Уточнение грубой версии. Нужно переходить к пределу рассматривая все более и более мелкие $A_{i}$. Легко перейти на счетно-порожденные \s-алгебры. Гораздо трудно перейти на \s-алгебры не являющиеся счетно-порожденными (например такой является хвостовая \s-алгебра).

\item Обобщение проекции. Проекция определена там, где есть углы и скалярные произведения, т.е. в $L^{2}$. Самая главная трудность состоит в переходе от $L^{2}$ определения к $L^{1}$ определению.

\item Решение системы уравнений. Никаких проблем, годится для общего случая. Но трудно доказать существование условного ожидания, т.к. в системе бесконечное количество уравнений, которое не сводится к конечному количеству. Мы пойдем этим путем. За доказательство существования будет отвечать т. Радона-Никодима.
\end{itemize}


Чаще всего приходится иметь дело не с абстрактной \s-алгеброй $\mathcal{H}$, а с \s-алгеброй, порожденной некоей случайной величиной $X$, $\mathcal{H}=\sigma(X)$. В этом случае используют вместо $\E(Y|\sigma(X))$ более простое обозначение $\E(Y|X)$. Если $X$ принимает конечное количество значений, то $\sigma(X)$ будет конечнопорожденной \s-алгеброй, и в этом случае можно искать $\E(Y|X)$ по формуле \ref{e_cond_simple}.

Мы интерпретируем $\E(Y|X)$ как среднее значение $Y$ при известном $X$. Значение $\E(Y|X)$ является функцией от $X$, а поскольку $X$ является случайной величиной, то и $\E(Y|X)$ является случайной величиной.


\subsection{Условное ожидание как случайная величина. Общий случай.}

\begin{mydef}
Пусть $\mathcal{H}$ - произвольная \s-алгебра и $Y$ - случайная величина с $\E(Y)<\infty$. Условным ожиданием $\E(Y|\mathcal{H})$ называется любая случайная величина $\hat{Y}$ удовлетворяющая двум условиям:

\begin{itemize}
\item $\hat{Y}$ является $\mathcal{H}$-измеримой
\item Для любого события $A\in\mathcal{H}$ верно равенство: $\E(\hat{Y}1_{A})=\E(Y1_{A})$
\end{itemize}
\end{mydef}

Отметим в частности, что в любую $\mathcal{H}$ входит множество $\Omega$, поэтому $\E(\hat{Y})=\E(Y)$.

Зачем в определении требуется, чтобы $\E(Y)<\infty$? Если этого не потребовать, могут возникнуть некоторые парадоксы.

Например: (две шкатулки)

Конечно же,
\begin{myth}
Если $\mathcal{H}$ - конечно порожденная \s-алгебра, то определения \ldots  и \ldots  совпадают.
\end{myth}
\begin{proof}

\end{proof}

Условные ожидания всегда существуют и почти наверное единственны:
\begin{myth}
Если $\E(Y)<\infty$ и $\mathcal{H}$ - \s-алгебра, то существует $\hat{Y}=\E(Y|\mathcal{H})$. Если еще и $\hat{Y}'=\E(Y|\mathcal{H})$, то $\P(\hat{Y}=\hat{Y}')=1$.
\end{myth}
\begin{proof}
\begin{itemize}
\item Существование. Если $Y\geq 0$, то на \s-алгебре $\mathcal{H}$ можно определить конечную меру $\mu(A):=\E(Y1_{A})$. На $\mathcal{H}$ эта мера абсолютно непрерывна по отношению к мере $P$. По теореме Радона-Никодима существует $\mathcal{H}$ измеримая случайная величина $\hat{Y}$, такая что $\mu(A)=\E(\hat{Y}1_{A})$. Для произвольного $Y$ существование доказывается путем разложения $Y=Y^{+}-Y^{-}$.
\item Единственность. Величины $\hat{Y}$ и $\hat{Y}'$ являются $\mathcal{H}$-измеримыми, поэтому $A=\{\hat{Y}-\hat{Y}'>\varepsilon\}\in\mathcal{H}$. $\E((\hat{Y}-\hat{Y}')1_{A})=\E(\hat{Y}1_{A})-\E(\hat{Y}'1_{A})=\E(Y1_{A})-\E(Y1_{A})=0$. С другой стороны, $\E((\hat{Y}-\hat{Y}')1_{A})\geq \E(\varepsilon 1_{A})=\varepsilon \P(A)$. Значит $\P(A)=0$ для всех $\varepsilon$. Событие $(\hat{Y}>\hat{Y}')=\cup_{\varepsilon=1/n}A(\varepsilon)$. В силу непрерывности вероятности, $\P(\hat{Y}>\hat{Y}')=0$. Из соображений симметрии, $\P(\hat{Y}'>\hat{Y})=0$.
\end{itemize}
\end{proof}

Условное ожидание обладает рядом хороших свойств:


\subsubsection*{Как считать условное ожидание?}

\begin{itemize}
\item Для конечной $\mathcal{H}$ ответ дает формула с которой мы начали определение условного ожидания:


\item Если нужно посчитать $\E(Y|X)$, где у пары случайных величин $X$ и $Y$ есть совместная функция плотности, то можно воспользоваться готовым рецептом:
\begin{myth}
Если у пары $X$, $Y$ есть совместная функция плотности $p(x,y)$, то условное ожидание $\E(Y|X)$ можно найти по формуле:
\begin{equation}
\E(Y|X)=\int_{-\infty}^{+\infty}y\cdot p(y|X)dy
\end{equation}
где $p(y|x)$ - условная функция плотности, $p(y|x)=\frac{p(x,y)}{p(x)}$.
\end{myth}
\begin{proof}

\end{proof}

\item В остальных случаях остается одно. Думать. Иногда из интуитивных или геометрических соображений можно догадаться до правильного ответа и тогда остается лишь проверить, что догадка $\hat{Y}$ удовлетворяет определению условного ожидания. При этом может помочь тот факт, что при проверке равенства $\E(\hat{Y}1_{A})=\E(Y1_{A})$ не обязательно проверять все $A\in\mathcal{H}$:
\begin{myth}
Пусть $\mathcal{M}$ - набор событий замкнутый относительно пересечений (если $A$, $B$ лежат в $\mathcal{M}$, то и $A\cap B$ лежит в $\mathcal{M}$), $\mathcal{H}=\sigma(\mathcal{M})$, $\E(Y)=\E(\hat{Y})<\infty$. Если $\E(\hat{Y}1_{A})=\E(Y1_{A})$ для всех $A\in\mathcal{M}$, то $\E(\hat{Y}1_{A})=\E(Y1_{A})$ для всех $A\in\mathcal{H}$.
\end{myth}
\begin{proof}
Рассмотрим набор множеств $\mathcal{N}$, для которых $\E(\hat{Y}1_{A})=\E(Y1_{A})$. Мы докажем, что $\mathcal{N}$ - \s-алгебра. Набор множеств $\mathcal{N}$ содержит $\mathcal{M}$ по условию. Если $\mathcal{N}$ - действительно \s-алгебра, то она обязана содержать наименьшую \s-алгебру $\mathcal{H}$ порождаемую набором $\mathcal{M}$. А это нам и требуется доказать.

По условию в $\mathcal{N}$ входит $\Omega$. Если $A\in\mathcal{N}$, то и $A^{c}\in\mathcal{N}$, т.к. $\E(Y1_{A^{c}})=\E(Y)-\E(Y1_{A})=\E(\hat{Y})-\E(\hat{Y}1_{A})=\E(\hat{Y}1_{A^{c}})$.

Если $A_{i}\in\mathcal{N}$, то и $\cup A_{i}\in\mathcal{N}$. Для непересекающихся $A_{i}$ это следует в силу DCT. а от пересекающихся можно легко перейти к непересекающимся. Здесь требуется замкнутость набора $\mathcal{M}$.
\end{proof}

Например, в самом распространенном случае, при поиске $\E(Y|X)$, т.е. когда $\mathcal{H}=\sigma(X)$, достаточно проверять не все множества из $\sigma{X}$, а только множества вида $X\leq t$. Вспомните теоремку \ref{generate_borel}.



\end{itemize}

Примеры\ldots


\subsubsection*{Условное ожидание как функция и парадокс Колмогорова-Бореля}

Образно говоря, «при прогнозировании монеток не подкидывают»:
\begin{myth} \label{e_cond_as_f}
Случайная величина $\E(Y|X)$ представима в виде $\E(Y|X)=f(X)$, где $f$ - неслучайная борелевская функция.
\end{myth}
\begin{proof}

\end{proof}
Эта теорема говорит, что вся случайность величины $\E(Y|X)$ связана только со случайной величиной $X$. Случайная величина $\E(Y|X)$ не может использовать другие источники случайности, не связанные с $X$!

Можно нарисовать такую схему:

% \Omega-> R (X)
% \Omega-> R {\E(Y|X)
% в осях \E(Y|X), X - график f


Уточним, что $f$ не обязательно единственна! Можно лишь утверждать, что если $f_{1}$ и $f_{2}$ две подобные функции, то $f_{1}(X)=f_{2}(X)$ почти наверное.

Очень часто хочется говорить о величине $\E(Y|X=x)$ (среднее значение $Y$, если $X=x$). Если $\P(X=x)>0$, то тут нет никаких проблем. Если $\P(X=x)=0$, то можно прибегнуть к представлению $\E(Y|X)=f(X)$. А именно, сказать что, по определению $\E(Y|X=x):=f(x)$. Например, для непрерывных $X$ и $Y$ хочется определить:

\begin{equation}
\E(Y|X=x)=\int_{-\infty}^{+\infty}yf(y|x)dy
\end{equation}

Это интуитивно понятно, но не стоит этим злоупотреблять! Дело в том, что функция $f$ определена не однозначно, а только почти наверное. Это означает, что для каждого конкретного $x$ можно выбрать любое $f(x)$! Именно поэтому предлагаемое «определение» не устойчиво к преобразованиям.

\begin{myex} Парадокс Колмогорова-Бореля.
Пусть совместная функция плотности $X$ и $Y$ имеет вид:
\begin{equation}
p_{X,Y}(x,y)=\left\{
\begin{array}{ll}
  1, & 0<y<1, -y<x<1-y \\
  0, & otherwise \\
\end{array}
\right.
\end{equation}

Пусть $U=\frac{X}{Y}+1$. Найдите:

а) $p_{Y|X}(y|x)$

б) $p_{U,Y}(u,v)$ и $p_{Y|U}(y,u)$

в) Верно ли, что $X=0 \Leftrightarrow U=1$?

г) Верно ли, что $p_{Y|X}(t|0)=p_{Y|U}(t,1)$?

д) Мораль?

\end{myex}







\subsubsection*{Условное ожидание и независимость}

Как связаны условное ожидание и независимость?

Из независимости $ X $ и $ \sigma $-алгебры $ \mathcal{H} $ следует, что $ \E(X|\mathcal{H})=\E(X)$. Обратное неверно, как показывает:
\begin{myex}
Пусть $ X\sim N(0;1) $. Зависимы ли $ X $ и $ X^{2} $? Найдите $ \E(X|X^{2}) $.

Решение. Очевидно зависимы, но $ \E(X|X^{2})=0=\E(X) $
\end{myex}

Тем не менее, независимость и условные ожидания тесно связаны:
\begin{myth}
Случайные величины $ X $ и $ Y $ независимы если и только если $ \E(f(X)|Y)=\E(f(X))$ для любой борелевской функции $f$.
\end{myth}
\begin{proof}
Если $ X $ и $Y  $ независимы, то независимы $f(X)  $ и $ Y $. Значит $ \E(f(X)|Y)=\E(f(X))$.

\ldots

\end{proof}

А именно, если $X$ и $Y$ независимы, можно ли утверждать, что $\E(X|\mathcal{H})$ и $\E(Y|\mathcal{H})$ независимы?

Интуитивно на этот вопрос легко ответить используя геометрическую интерпретацию. Пусть есть две некоррелированные случайные величины $X$ и $Y$. На рисунке (???) они изображены как перпендикулярные векторы. Могут ли их проекции на какую-нибудь плоскость не быть перпендикулярны? Да, конечно могут! Скорее наоборот, чтобы между проекциями получился прямой угол нужно выполнения особого условия (???) вектор $X-Y$ должен лежать в плоскости (??? проверить утверждение)

В качестве конкретного примера подойдет практически первый попавшийся! Например, правильную монетку подбрасывают 20 раз, $X$ - число орлов в первых 10 подбрасываниях, $Y$ - число орлов в последних 10 подбрасываниях, $Z$ - в подбрасываниях с 6-го по 15-ое. Очевидно, что $X$ и $Y$ независимы, однако $\E(X|Z)=\E(Y|Z)=\frac{Z}{2}+\frac{5}{2}$.

А может быть из независимости $X$ и $Y$ будет следовать хотя бы $\E(XY|\mathcal{H})=\E(Y|\mathcal{H})\cdot \E(Y|\mathcal{H})$, «условная некоррелированность»?

Предыдущий пример также показывает, что это не так. Разложив $X$ и $Y$ в сумму индикаторов, можно установить, что $\E(XY|Z)=25\left(\frac{Z(Z+8)}{90}+\frac{1}{4}\right)\neq \left(\frac{Z}{2}+\frac{5}{2}\right)^{2}$.



% Хотя кое-какая еще связь есть\ldots  (откуда я помню дикую формулу и куда ее применить?)




\subsubsection*{Задачи}


\begin{problem}
Пусть $\hat{Y}=\E(Y|\mathcal{H})$, где $\mathcal{H}$ - произвольная \s-алгебра. Пусть $X$ - $\mathcal{H}$-измеримая случайная величина, принимающая конечное множество значений. Докажите, что $Cov(X,\hat{Y})=Cov(X,Y)$
\begin{sol}
$X$ допускает разложение $X=\sum x_{i}1_{A_{i}}$. В силу линейности ковариаций достаточно проверить равенство для индикаторов. Проверяем: $Cov(1_{A_{i}},\hat{Y})=\E(\hat{Y}1_{A_{i}})-\E(\hat{Y})\E(1_{A_{i}})=\E(Y1_{A_{i}})-\E(Y)\E(1_{A_{i}})=Cov(1_{A_{i}},Y)$.
\end{sol}
\end{problem}


\begin{problem}
Приведите пример, таких $Y_{1}$, $Y_{2}$ и $\mathcal{H}$, что $\E(Y_{1}|\mathcal{H})$ $\E(Y_{2}|\mathcal{H})$ независимы, а $Y_{1}$ и $Y_{2}$ зависимы.
\begin{sol}
Например, пусть $Y_{1}$ - распределено геометрически и $Z$, независимо от $Y_{1}$, принимает равновероятно значения $+1$ или $-1$. Определим $Y_{2}:=ZY_{1}$ и $\mathcal{H}:=\sigma(Y_{1})$. Получаем, что $Y_{1}=|Y_{2}|$, но $\E(Y_{2}|\mathcal{H})=0$, поэтому $\E(Y_{1}|\mathcal{H})$ и $\E(Y_{2}|\mathcal{H})$ независимы.
\end{sol}
\end{problem}




\subsection{Еще задачи}


\Closesolutionfile{solution_file}
