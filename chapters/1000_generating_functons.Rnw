\Opensolutionfile{solution_file}[sols_chap_10]
% в квадратных скобках фактическое имя файла

\section{Производящие функции}

\url{http://www.stats.uwaterloo.ca/~dlmcleis/s901/s901_2005.pdf}
\url{http://www.mathematik.uni-muenchen.de/~lerdos/WS04/FA/compl.pdf}

Если есть случайная величина $X$, то как описать ее закон распределения?

Есть много способов! Если величина непрерывная, то можно описать ее с помощью функции плотности, $p(t)$. Если случайная величина дискретная --- с помощью функции $p(t):=\P(X=t)$. Есть универсальный способ записи, подходящий для любых случайных величин --- функция распределения, $F(t):=\P(X\leq t)$. В конце концов можно описать просто текстом: например, «правильную» монетку подкидывают $n$ раз и $X$ - это количество «орлов».

Сейчас мы познакомимся еще с одной идеей, позволяющей описать закон распределения случайной величины.

Эта идея будет представлена тремя функциями:

\begin{itemize}
\item Производящая функция, $ g_{X}(t):=\E(t^{X}) $
\item Функция производящая моменты, $ m_{X}(t):=\E(e^{tX}) $
\item Характеристическая функция, $ \phi_{X}(t):=\E(e^{itX}) $
\end{itemize}


\subsection{Производящая функция}


Если $ X $ --- целочисленная неотрицательная случайная величина, то для ее описания подходит

\begin{mydef}
Функция $ g_{X}(t):=\E(t^{X}) $ называется производящей функцией случайной величины $ X $.
\end{mydef}


Пример с табличкой. (\ldots )


Найдем производящую функцию для пуассоновской случайной величины.

$ g_{X}(t)=\E(t^{X})= $



Производящая функция может не существовать\ldots

Пример. \ldots .



По производящей функции можно «опознать» случайную величину:
\begin{myth}
Если совпадают производящие функции $ g_{X}(t) $ и $ g_{Y}(t) $, то случайные величины $ X $ и $ Y $ имеют одинаковое распределение.
\end{myth}


Легко найти производящую функцию суммы:
\begin{myth}
Если существуют $ g_{X}(t) $ и $ g_{Y}(t) $, то :
\[g_{X+Y}(t)=g_{X}(t)\cdot g_{Y}(t)\]
\end{myth}


С помощью производящей функции легко считать математическое ожидание и другие моменты:
\begin{myth}
Если существуют $ g_{X}(t) $ и $ \E(X) $, то:

\[ \E(X)=g'_{X}(1) \]

\[ \E(X(X-1))=g''_{X}(1) \]

\[ \E(X(X-1)(X-2))=g'''_{X}(1) \]

\ldots

\end{myth}


Найдем производящую функцию для биномиальной случайной величины.

Упражнение. Пусть $ X \sim Bin(n,p)$. Найдите $ g_{X}(t) $.

Решение. Биномиальная случайная величина представима в виде суммы независимых $ X=X_{1}+X_{2}+\ldots +X_{n} $. Находим производящую функцию отдельного слагаемого, получаем $ g_{X_{i}}(t)=pt+1-p $. Значит для биномиальной случайной величины $ g_{X}(t)=(pt+1-p)^{n} $




Теперь попробуем применить производящую функцию на реальных задачах:

Пример.






Сейчас мы познакомимся еще с одним способом описать закон распределения случайной величины, но для этого нам потребуются комплексные случайные величины.





\subsection{Комплексные случайные величины}

Для работы с характеристическими функциями нам потребуются комплексные случайные величины.

\begin{mydef}
\indef{Математическим ожиданием} комплексной случайной величины $Z$ называется число
\begin{equation}
z:=\E(Re(Z))+i\E(Im(Z))
\end{equation}
при условии, что $\E(|Z|)<\infty$.

\end{mydef}


У математического ожидания по-прежнему сохраняются все полезные свойства:
\begin{itemize}
\item $\E(cZ)=c\E(Z)$, если $c\in\mathbb{C}$
\item $\E(Z_{1}+Z_{2})=\E(Z_{1})+\E(Z_{2})$, если мат. ожидания существуют

\end{itemize}





\subsection{Определение и примеры}

Однако есть принципиально другой подход к описанию закона распределения случайной величины, а именно - производящие/характеристические функции. Их много видов:

\begin{itemize}
\item Производящая функция, $g(t)=\E(t^{X})$
\item Функция производящая моменты, $m(t)=\E(e^{tX})$
%\item Преобразование Лапласа, $L(t)=\E(e^{-tX})$
%\item Преобразование Фурье, $\E(e^{-itX})$
\item Характеристическая функция, $\phi(t)=\E(e^{itX})$.
\end{itemize}

Чем они хороши?

\begin{itemize}
\item Любой случайной величине (дискретной, непрерывной\ldots ) сопоставляется непрерывная функция!
\item Во-первых, они подходят для описания закона распределения случайной величины. Если у величин $X$ и $Y$ совпадают производящие функции, то у них совпадает закон распределения.
\item Если $X$ и $Y$ - независимые случайные величины, то производящая функция суммы легко находится: $g_{X+Y}(t)=g_{X}(t)g_{Y}(t)$. Для сравнения: при использовании функций плотности придется считать интеграл!
\item С помощью них легко считать моменты случайной величины $\E(X^{k})$, при этом нужно будет только находить производную.
\item Некоторые из них, а именно преобразование Фурье и характеристическая функция существуют для любых случайных величин.
\end{itemize}

Чем они плохи?
\begin{itemize}

\item Их труднее интерпретировать. Мы постараемся\ldots
\item Вероятности с их помощью считать труднее.
\item Многие студенты в 21 веке до сих пор боятся комплексных чисел\footnote{Впрочем, это скорее недостаток студента, а не характеристической функции}.
\end{itemize}

Давайте попробуем посчитать и убедимся, что все не так трудно!
\begin{myex} Пусть $X$ принимает значения $-1$, $2$ и $4$ с вероятностями $0.1$, $0.4$ и $0.5$ соответственно. Найдите
\begin{enumerate}
\item Производящую функцию
\item Функцию производящую моменты
\item Характеристическую функцию
\end{enumerate}
Выписав значения соответствующих величин получаем:
\begin{enumerate}
\item $g_{X}(t)=0.1t^{-1}+0.4t^{2}+0.5t^{4}$,
\item $m_{X}(t)=0.1e^{-t}+0.4e^{2t}+0.5e^{4t}$,
\item $\phi_{X}(t)=0.1e^{-it}+0.4e^{2it}+0.5e^{4it}$. У характеристической функции можно выделить мнимую и действительную части: $\phi_{X}(t)=0.1\cos(t)+0.4\cos(2t)+0.5\cos(4t)+i(-0.1\sin(t)+0.4\sin(2t)+0.5\sin(4t))$.
\end{enumerate}

\end{myex}

Уже можно заметить, что производящие функции оказались неограниченными, а характиристическая - ограничена! Еще один пример:

\begin{myex} Какие значения с какими вероятностями принимает случайная величина $X$ если:

\begin{enumerate}
\item Производящая функция равна $g_{X}(t)=0.6t^{-5}+0.4t^{7}$
\item Функция производящая моменты равна $m_{X}(t)=0.7e^{3t}+0.3e^{27t}$
\item Характеристическая функция равна $\phi(t)=0.5(1+\cos(t))+i\cdot 0.5\sin(t)$.
\end{enumerate}
Ответы, конечно, читаются в условии:

\begin{enumerate}
\item $X$ принимает значения $-5$ и $7$ с вероятностями $0.6$ и $0.4$ соответственно.
\item $X$ принимает значения $3$ и $27$ с вероятностями $0.7$ и $0.3$ соответственно.
\item Заметим, что $1=\cos(0\cdot t)$ и $0=\sin(0\cdot t)$, поэтому наша функция равна $phi_{X}(t)=0.5e^{i\cdot 0\cdot t}+0.5e^{i\cdot 1\cdot t}$. Значит $X$ равновероятно принимает значения $0$ и $1$.
\end{enumerate}

В этом примере есть маленькая нестрогость: мы нашли случайную величину с данной производящей функцией, но не доказали, что не существует другой.
\end{myex}

\begin{myex} Пример, когда $g(t)$ не существует
\end{myex}

Мы остановимся более подробно на характеристической функции, так как она определена для любой случайной величины.

Для начала маленькое напоминание про математическое ожидание комплексных случайных величин. Если $Z=X+iY$ - комплексная случайная величина, то (по определению), $\E(Z)=\E(X)+i\E(Y)$. Заметим также, что $e^{itX}=\cos(tX)+i\sin(tX)$. Поэтому, под выражением $\E(e^{itX})$ нужно понимать не что иное как $\E(\cos(tX))+i\E(\sin(tX))$. Из этого сразу следует, что характеристическая функция всегда существует. Действительно, косинус и синус ограничены, значит оба математических ожидания всегда существуют.

\begin{myex} Пусть $X$ равномерна на $[0;1]$. Найдем характеристическую функцию: $\phi(t)=\E(e^{itX})=\E(\cos(tX)+i\sin(tX))=\int_{0}^{1}\cos(tx)dx+i\int_{0}^{1}\sin(tx)dx=\frac{\sin(t)}{t}+i\frac{\cos(t)-1}{t}$. При $t=0$, особый, но простой случай, $\phi(0)=1$.
\end{myex}

Все, конечно, хорошо представляют как выглядит график функции плотности равномерной случайной величины:
(\ldots )

А как представить графически характеристическую функцию? Можно поступить несколькими способами:

\begin{itemize}
\item Нарисовать кривую в 3-х мерном пространстве (рисунок)
\item Нарисовать два графика на плоскости: $Re(\phi(t))$ и $Im(\phi(t))$:
\item Нарисовать два графика на плоскости: $|\phi(t)|$ и $\arg(\phi(t))$ (в качестве аргумента комплексного числа можно выбрать любую версию, например попадающую в интервал $[0;2\pi)$ или непрерывную)
\end{itemize}

Второй способ, по всей видимости, предпочтительнее так как в случае, когда закон распределения случайной величины $X$ симметричен относительно нуля (т.е. $\P(X\leq t)=\P(X\geq -t)$ для $\forall t$), оказывается, что $Im(\phi(t))=0$ и достаточно одного графика. Чуть позже мы докажем это, а пока еще один пример.

\begin{myex} Случайная величина $X$ принимает равновероятно значения $1$ и $(-1)$. Как выглядит ее характеристическая функция? $\phi(t)=\E(e^{itX})=0.5e^{-it}+0.5e^{it}=0.5(\cos(-t)-i\sin(t))+0.5(\cos(t)+i\sin(t))=\cos(t)$. Как и было обещано, мнимая часть отсутствует.
\end{myex}




\subsection{Свойства характеристической функции}

\begin{itemize}
\item $\phi(0)=1$

По определению, $\phi(0)=\E(\cos(0\cdot X))+i\E(\sin(0\cdot X))=1$

\item $|\phi(t)|\leq 1$ для любого $t$

$|\phi(t)|=|\E(\cos(t\cdot X)+i\sin(t\cdot X))|\leq \E(|\cos(t\cdot X)+i\sin(t\cdot X)|)=1$
\item $\phi_{-X}(t)=(\phi_{X}(t))$- сопряжение поставить!

\item $\phi_{aX+b}(t)=e^{iat}\phi(bt)$


\item Если $X$ и $Y$ независимы, то $\phi_{X+Y}(t)=\phi_{X}(t)\phi_{Y}(t)$

\begin{multline}
\phi_{X+Y}(t)=E\left(e^{it(X+Y)}\right)=E\left(e^{itX}e^{itY}\right)\\
=E\left(e^{itX}\right)E\left(e^{itY}\right)=\phi_{X}(t)\phi_{Y}(t),
\end{multline}
в силу независимости $e^{itX}$ и $e^{itY}$

\item Если закон распределения $X$ симметричен, то $Im(\phi_{X}(t))=0$
\item Функция $\phi$ является равномерно непрерывной
\end{itemize}



\subsection{Доказательство ЦПТ}



\subsection{Поиск вероятностей и среднего с помощью характеристической функции}

Допустим нам нужно найти вероятность связанную с суммой случайных величин. В силу того, что для суммы легко ищется характеристическая функция, бывает что проще искать вероятность именно с ее помощью.


Пример. Вероятность вымирания. \par

Еще Пример. Поиск вероятности с использованием производящей функции.\par
Мы рассмотрим пример с векторными случайными величинами.


Пример. Мы подбрасываем монетку до тех пор, пока количество орлов не привысит количества решек. $T$ - количество подбрасываний. Найдите $\E(1/T)$.

Заметим, что $1/T=\int_{0}^{1}x^{T-1}dx$.

Ссылаясь на теорему \ldots  $\E(1/T)=\int_{0}^{1}\E(x^{T-1})dx$.

А $\E(x^{T})$ - это не что иное, как производящая функция!



Из этого примера следует мораль: производящие/характеристические функции дают возможность заменить дискретную сумму на интеграл непрерывной функции!

Хотите поговорить об этом?

\subsection{Суммирование рядов с помощью производящих функций\ldots }


основное из статьи A unified method to sum a variety of infinite series with applications



\subsection{Еще задачи}

\Closesolutionfile{solution_file}
