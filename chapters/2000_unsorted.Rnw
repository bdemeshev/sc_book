\Opensolutionfile{solution_file}[sols_chap_20]
% в квадратных скобках фактическое имя файла

\section{Неразобрано!}


Глава 2. Сигма-алгебры, измеримость. \par

Знаком $\sigma(\mathcal{E})$ мы будем обозначать минимальную
$\sigma$-алгебру, порожденную множеством $\mathcal{E}$


\begin{mydef}
Пусть $\Omega=\mathbb{R}$, и $\mathcal{E}=\{$Открытые
множества$\}$. \textit{Борелевской} $\sigma$-алгеброй
$\mathcal{B}$ называется $\mathcal{B}=\sigma(\mathcal{E})$
\end{mydef}
Элементы этой $\sigma$-алгебры называются борелевскими
множествами.



\begin{myth}
Пусть $\mathcal{A}=\{$Множества вида $(-\infty ;$ t$)\}$, и
$\mathcal{E}=\{$Открытые множества$\}$. Тогда
$\sigma(\mathcal{A})=\sigma(\mathcal{E})$.
\end{myth}
Из $\mathcal{A}\subseteq\mathcal{E}$ следует, что
$\sigma(\mathcal{A})\subseteq\sigma(\mathcal{E})$

Выберем произвольное открытое $A$. Доказательство того, что
 $A\in\sigma(\mathcal{A})$ разобьем на три этапа:

Во-первых, множества вида $[a;b)\in\sigma(\mathcal{A})$ в силу
того, что $[a;b)=(-\infty;b)\bigcap(\Omega \backslash
(-\infty;a))$

Во-вторых, построим последовательность множеств $A_{i}$.

$A_{1}$ будет состоять из полуинтервалов длины 1 вида $[n;n+1)$,
целиком содержащихся в $A$.

$A_{2}$ будет состоять из полуинтервалов длины $1/2$ вида
$[\frac{n}{2};\frac{n+1}{2})$, целиком содержащихся в $A$.

Каждое $A_{i}$ лежит в $\sigma(\mathcal{A})$

В-третьих, $A=\bigcup_{i=1}^{+\infty}A_{i}$





\begin{myth}
Если $\sigma(\mathcal{E})=\mathcal{B}$, то для того,
чтобы $f$ была измеримой, необходимо и достаточно, чтобы $\forall
A\in \mathcal{E}\Rightarrow$ $f^{-1}(A)\in\mathcal{F}$
\end{myth}
\begin{myth}
Если $X$ - случайная величина, а $f$ - борелевская
функция, то $f\circ X()=f(X(\cdot))$ - случайная величина.
\end{myth}
Пример. Если $X$ - случайная величина, то и $X^{2}$ - случайная
величина.


Корректно говорить, что функций $f$ является
$\mathcal{F}$-измеримой. Указывать о какой $\sigma$-алгебре идет
речь необходимо потому, что одна и та же функция может быть
измеримой относительно одной $\sigma$-алгебры и быть неизмеримой
относительно более бедной $\sigma$-алгебры. Однако, если из
контекста понятно, о какой $\sigma$-алгебре идет речь, мы будем
пропускать явное указание.



Идея replicating portfolio. \par
Пример (оценить 75 центов в случае > 1 - см. Crack)


Теорема. Формула Блэка-Шоулса в непрерывном времени как предел дискретной формулы. \par




Независимость сигма-алгебр. Независимость случайных величин. Из
независимости $X$ и $Y$ следует независимость $f(X)$ и $g(Y)$.


Для более удобной работы используются такие обозначения:


$A_{i}\uparrow A$ - $A_{i}$ "стремится снизу" к $A$. Это означает,
что $A_{1} \subseteq A_{2} \subseteq A_{3} \subseteq \ldots $ и
$\bigcup A_{i}=A$.

$A_{i}\downarrow A$ - $A_{i}$ "стремится сверху" к $A$. Это
означает, что $A_{1} \supseteq A_{2} \supseteq A_{3} \supseteq
\ldots $ и $\bigcap A_{i}=A$.


Исходя из свойств P1-P3 можно вывести, что у вероятности также
имеется ряд других удобных свойств.

Если $A\in\mathcal{F}$, $B\in\mathcal{F}$, $\forall
A_{i}\in\mathcal{F}$, то выполнены, например, следующие свойства:

1. $\mathbf{P}(\overline{A})=1-\mathbf{P}(A)$

2. Если $B\subseteq A$, то $\mathbf{P}(B)\leq \mathbf{P}(A)$

3. $\mathbf{P}(A\bigcup
B)=\mathbf{P}(A)+\mathbf{P}(B)-\mathbf{P}(A\bigcap B)$

4. $\mathbf{P}(\bigcup_{i=1}^{+\infty} A_{i})=lim_{n\rightarrow
+\infty} \mathbf{P}(\bigcup_{i=1}^{n} A_{i})$

5. $A_{i} \uparrow A \Rightarrow \mathbf{P}(A)=lim_{i\rightarrow
+\infty} \mathbf{P}(A_{i})$

6. $A_{i} \downarrow A \Rightarrow \mathbf{P}(A)=lim_{i\rightarrow
+\infty} \mathbf{P}(A_{i})$


\begin{mydef}
Измеримое пространство - это набор
$(\Omega,\mathcal{F})$
\end{mydef}

\begin{mydef}
Вероятностное пространство - это набор
$(\Omega,\mathcal{F},\mathbf{P})$
\end{mydef}

Пополнение.

Пусть $(\Omega,\mathcal{F},\mathbf{P})$ - вероятностное
пространство, и $\mathbf{P}(A)=0$. Иногда может найтись такое
множество $C$, что $C$ неизмеримо, но $C \subseteq A$. Таких
ситуаций хотелось бы избежать, для этого существует процедура
пополнения (completion) вероятностного пространства.

Для начала определим набор множеств $\mathcal{N}=\{C \notin
\mathcal{F} |\exists A\supseteq C, \mathbf{P}(A)=0\}$.

$\mathcal{N}$ не является алгеброй. Например, $\Omega \notin
\mathcal{N}$.

Определим набор множеств $\mathcal{\overline{F}}=\{F\bigcup N | F
\in \mathcal{F}, N \in \mathcal{N}\}$.

Для $\overline{F}=F\bigcup N$ определим
$\mathbf{\overline{P}}(\overline{A})=\mathbf{P}(F)$.

Докажем, что $\mathcal{\overline{F}}$ - $\sigma$-алгебра:

Во-первых, $\emptyset\in \mathcal{\overline{F}}$  в силу того, что
$\emptyset=\emptyset\bigcup\emptyset$, где
$\emptyset\in\mathcal{F}$ и $\emptyset\in\mathcal{N}$.

Во-вторых,


В-третьих,
$$
\bigcup_{i}\left(F_{i}\bigcup N_{i}\right)=\left(\bigcup_{i}
F_{i}\right)\bigcup\left(\bigcup_{i} N_{i}\right)
$$
При этом $\bigcup_{i} F_{i}$ измеримо и $\bigcup_{i} N_{i} \in
\mathcal{N}$.

Докажем, что $\mathbf{\overline{P}}$ - вероятность.

Сначала установим, что определение внутренне непротиворечиво.

Допустим в $\mathcal{\overline{F}}$ появится некоторое множество
допускающее два разложения $C=F_{1}\bigcup N_{1}=F_{2}\bigcup
N_{2}$, где $N_{1}, N_{2} \in \mathcal{N}$, а $F_{1}, F_{2} \in
\mathcal{F}$, причем $\mathbf{P}(F_{1})\neq \mathbf{P}(F_{2})$.

Заметим, что $F_{1} \subset F_{2}\bigcup N_{1} \bigcup N_{2}$,
следовательно, $F_{1} \setminus F_{2} \subset (F_{2}\bigcup N_{1}
\bigcup N_{2}) \setminus F_{2}$.

Но $(F_{2}\bigcup N_{1} \bigcup N_{2})\setminus F_{2} \in
\mathcal{N}$, следовательно, $\mathbf{P}(F_{1} \setminus
F_{2})=0$.

Далее воспользовавшись тем, что $F_{1}=(F_{1}\bigcap
F_{2})\bigcup(F_{1} \setminus F_{2})$ получаем, что
$\mathbf{P}(F_{1})=\mathbf{P}(F_{1}\bigcap F_{2})$.

По аналогичному рассуждению получаем, что
$\mathbf{P}(F_{2})=\mathbf{P}(F_{1}\bigcap F_{2})$, что доказывает
непротиворечивость определения новой вероятности.

Остается доказать, что $\mathbf{P}$ является $\sigma$-аддитивной.
$\mathbf{\overline{P}}(\bigcup_{i}(F_{i}\bigcup
N_{i}))=\mathbf{\overline{P}}((\bigcup_{i}
F_{i})\bigcup(\bigcup_{i} N_{i}))=\mathbf{P}(\bigcup_{i} F_{i})$


Маленькая сенсация!

Оказывается, «почти» - это строгий математический термин.

Говорят, что некоторое свойство выполнено почти наверное (almost
surely, a.s.), если существует событие с вероятностью единица, на
котором свойство выполнено.

Например, $X=Y$ a.s. означает, что $\mathbf{P}(X\neq Y)=0$.

Понятие «почти наверное» играет важную роль в силу того, что
случайные величины равные почти наверное не отличимы по
большинству характеристик. Многие интуитивные выводы становятся
абсолютно точными, если к ним добавлять «почти наверное».
Например, (скорее всего, это будет упражнением через лекцию или
две) из того, что $Var(X)=0$ следует, что $X=const$ почти
наверное.


Итак \textbf{Каратеодори} - человек и теорема!

Пусть задан набор $(\Omega,\mathcal{A},\mathbf{P})$, где
$\mathcal{A}$ - алгебра, а $\mathbf{P}$ - вероятность, заданная на
алгебре.

Вспомним, что мы определяли $\mathbf{P}$ на $\sigma$-алгебре. Там
требовалось выполнение свойств:

P1. $\mathbf{P}(A)\geq$ $0$

P2. Если $A_{i}$ попарно не пересекаются, то $\mathbf{P}(\bigcup
A_{i})=\sum \mathbf{P}(A_{i})$

P3. $\mathbf{P}(\Omega)=1$

Т.к. $\mathcal{A}$ - всего лишь алгебра, то в пункте P2 нужно
добавить условие $\bigcup_{i} A_{i} \in \mathcal{A}$. В
$\sigma$-алгебре оно было бы выполнено само собой.

P2': Если $A_{i}$ попарно не пересекаются и $\bigcup_{i} A_{i} \in
\mathcal{A}$, то $\mathbf{P}(\bigcup A_{i})=\sum
\mathbf{P}(A_{i})$






Глава 6. Интеграл Лебега. \par

«Четыре шага» \par
Для индикатора. \par
Для простой. \par
Для неотрицательной. \par
Для любой действительной. \par

FL. Fatou's Lemma. \par
Если $X_{n}\ge 0$, то
$$
E\left(\liminf_{n\rightarrow +\infty}X_{n}\right)\le
\liminf_{n\rightarrow +\infty}\E(X_{n})
$$

MCT. Monotone Convergence Theorem. \par
Если $X_{n+1} \ge X_{n} \ge 0$, то
$$
\lim_{n\rightarrow +\infty}\E(X_{n})=E\left(\lim_{n\rightarrow
+\infty}X_{n}\right)
$$

DCT. Dominated Convergence Theorem. \par
Если $X_{n}\rightarrow X$ п.н. и $|X_{n}|<Y$, где $\E(Y)<+\infty$,
то $\E(|X|)<+\infty$ и
$$ \lim_{n\rightarrow +\infty}\E(X_{n})=\E(X) $$
$$ \lim_{n\rightarrow+\infty}\E(|X_{n}-X|)=0 $$

В MCT и FL пределы могут принимать значение $+\infty$. \par

BCT. Bounded Convergence Theorem. \par
Если $X_{n}\rightarrow X$ (in P) и $|X_{n}|<M$, то
$\lim_{n\rightarrow +\infty}\E(X_{n})=\E(X)$. \par


Примеры: \par
Мат. ожидание = $\int dP$ \par
Площадь под кривой = $\int d\lambda$ \par

Теорема: \par
Интеграл Римана = интеграл Лебега \par

Теорема: \par
Пусть с.в. $X$ индуцирует меру $Pr$ на прямой. Тогда $\int f(X)dP=\int fdPr$ \par
Док-во: «Четыре шага» \par

Определение. Функция плотности. \par

Теорема: \par
Если у с.в. $X$ есть функция плотности, то $\int fdPr=\int fpd\lambda$ \par
Док-во: «Четыре шага» \par

Задачи: \par
доказать MCT, DCT для почти наверное \par
доказать FL для вероятности \par


Теорема: Если $X$ и $Y$ независимы, то $\E(XY)=\E(X)\E(Y)$ \par
Док-во: «Четыре шага» \par


Неравенства: Jensen, Cauchy-Schwarz, Holder. \par
Jensen - да, остальные - ? \par

Теорема Фубини (4 шага, возможно шаг 1 неполностью) \par
Взятие производной под интегралом. \par





Глава 8. Условное ожидание. \par

Условное математическое ожидание. Мыслим как о проекции.
Формальное определение. Свойства \par
Теорема Пифагора! \par
Теорема Радона-Никодима (без док-ва?). \par


Свойства в куче: \par
1. Единственность почти наверное. Если $Y_{1}=\E(X|\mathcal{H})$ и $Y_{2}=\E(X|\mathcal{H})$ то $Y_{1}=Y_{2}$ ae \par
Пусть $X$, $X_{n}$ и $Y$ - интегрируемые (из $L^{1}$) случайные величины. \par
2. $ \E(\E(X|\mathcal{H}))=\E(X) $ \par
3. $ \mathcal{H}=\{\emptyset,\Omega\} \Rightarrow \E(X|\mathcal{H})=\E(X) $ \par
4. $ X \in \mathcal{H} \Rightarrow \E(X|\mathcal{H})=X $ (as) \par
5. $\E(aX+bY|\mathcal{H})=a\E(X|\mathcal{H})+b\E(Y|\mathcal{H}) $ \par
6. $X \ge 0 \text{(as)} \Rightarrow \E(X|\mathcal{H})\ge 0$ (as) \par
7. $ X \ge Y \text{(as)} \Rightarrow \E(X|\mathcal{H})\ge \E(Y|\mathcal{H})$ (as) \par
8. $ |\E(X|\mathcal{H})|\le \E(|X||\mathcal{H})$ (as)  \par
9. $ X \in \mathcal{H}, XY \in L_{1} \Rightarrow \E(XY|\mathcal{H})=X\E(Y|\mathcal{H})$ (as) \par
10. $ X_{n} \uparrow X \Rightarrow \E(X_{n}|\mathcal{H})\rightarrow
\E(X|\mathcal{H})$(as) (MCT) \par
11. Если $\mathcal{H}\subseteq \mathcal{F}$, то
$\E(X|\mathcal{H})=\E(\E(X|\mathcal{F})|\mathcal{H})$ (as) (Tower property, теорема о трех перпендикулярах) \par
12. Неравенство Йенсена \par
13. Если $X$ и $\mathcal{H}$ независимы, то $\E(X|\mathcal{H})=\E(X)$ ae \par
14. Геометрический смысл - проекция \par
$\E((X-Y)^{2})$ достигает своего минимума для $Y\in\mathcal{H}$ при $Y=\E(X|\mathcal{H})$ \par
15. Т. Пифагора $\E(X^{2})=\E(\E(X|\mathcal{H})^{2})+\E((X-\E(X|\mathcal{H}))^{2})$ \par

Док-во 9. \par
1. Пусть $X$ - неотрицательная. $Y$ - индикатор, $Y$ - простая, $Y$ - неотрицательная.\par
2. $X$ и $Y$ - любые. \par

Упражнения: \par












Глава 9. «Практическая».  \par
Метод первого шага. Задача про разборчивую невесту. \par
Первым шагом: \par
1. Вероятность достичь уровня A раньше, чем (-B) \par
2. Ожидаемое время достижения уровня А или уровня (-B) \par
3. Вероятность рано или поздно выигрыть 1 рубль. \par
4. Ожидаемое время до выигрыша в 1 рубль. \par
5. Ожидаемое время обнаружения пчел Винни-Пухом (время возвращения
пьяницы домой из кабака) \par
6. Вероятность возвращения когда-либо в точку старта \par
7. Задача про хрюшек-копилок \par
8. вероятность для асимметричного блуждания, \par

Марковские цепи. \par
Марковская цепь неформально (график состояний со стрелочками) \par
Что лучше ООР или РОО? \par



Глава 10. Мартингалы в дискретном времени. \par

Тема. Определение мартингала и момента остановки. Мартингальное
преобразование. Upcrossing inequality. \par

\textbf{Определение мартингалов}

Пусть задано вероятностное пространство
$(\Omega,\mathcal{F},\mathbf{P})$.

Определение. Фильтрацией называется последовательность вложенных
$\sigma$-алгебр:
$$
\mathcal{F}_{0} \subseteq \mathcal{ F }_{1} \subseteq \ldots
\subseteq \mathcal{F}_{n} \subseteq \ldots  \subseteq\mathcal{F}
$$

$\mathcal{F}_{n}$ - те события, которые мы отличаем в момент
времени $n$. \par
$\mathcal{F}$ - самая полная (подробная) $\sigma$-алгебра. Все
рассматриваемые величины являются $\mathcal{F}$-измеримыми. \par
$\mathcal{F}_{0}$ как правило состоит только из $\emptyset$ и
$\Omega$. \par

%Определение. Вероятностное пространство с фильтрацией (filtered
%probability space, stochastic basis). $(\Omega,\mathcal{F},\{
%\mathcal{F}_{n} \}, \mathbf{P})$. \par
Определение. Случайный процесс - это набор случайных величин,
заданных на одном вероятностном пространстве. \par
Определение. Случайный процесс $\{ X_{t} \}$ адаптирован к
фильтрации $\{ \mathcal{F}_{t} \}$ если $X_{t} \in
\mathcal{F}_{t}$ (условная запись для фразы "$X_{t}$ является
$\mathcal{F}_{t}$-измеримой случайной величиной"). \par
Определение. Естественная фильтрация (natural filtration).
$\mathcal{F}_{n}=\sigma(X_{0},X_{1},\ldots X_{n})$. \par
Определение. Интегрируемый адаптированный случайный процесс $X$
называется [по отношению к $\mathcal{F}_{t}$]: \par
Мартингалом (martingale): $\E(X_{t+1}|\mathcal{F}_{t})=X_{t}$ п.н. \par
Субмартингалом: $\E(X_{t+1}|\mathcal{F}_{t})\ge X_{t}$ п.н. \par
Супермартингалом: $\E(X_{t+1}|\mathcal{F}_{t})\le X_{t}$ п.н. \par

Мартингал полезно понимать, как денежное благосостояние игрока
после $n$ раундов честной игры. \par
Субмартингал соответствует игре в пользу игрока. \\
С точки зрения игрока, в супермартингале нет ничего "суперского"
:) \\


% Откуда происходит название? \par
% Определение. Функция $f(x_{1},\ldots ,x_{n})$ называется
% супергармонической, если у нее есть непрерывные производные
% первого и второго порядков и $\partial^{2}f/\partial
% x_{1}^{2}+\ldots +\partial^{2}f/\partial x_{n}^{2}\le 0$. \par
% Теорема (без доказательства). Если $f(x)$ супергармоническая, то
% $$
% f(x) \ge \frac{1}{|B(0,r)|} \int_{B(x,r)}f(y)dy
% $$
% Упражнение. Пусть $X_{1}$, $X_{2}$, \ldots  - iid, равномерны в шаре
% $B(0,1)$. Определим $S_{n}=S_{n-1}+X_{n}$. Докажите, что
% $M_{n}=f(S_{n})$ - супермартингал, если $f(x)$ -
% супергармоническая. \par
% Происхождение названия 'супергармонической' функции в этом курсе останется нераскрытым :)

Обозначения. Вместо формального $\{ X_{n} \}_{n=0,1,2\ldots }$ мы
будем писать просто $X_{n}$ или даже $X$. \par

Упражнение 1. \par
Докажите, что если $X$ - мартингал, то \par
1. $\E(X_{n}|\mathcal{F}_{m})=X_{m}$ п.н. для $m\le n$ \par
2. $\E(X_{n+1}) = \E(X_{n})$ \par
Докажите аналогичные свойства для суб- и супермартингалов \par

Упражнение 2. \par
Докажите, что если процесс $X$ - супермартингал по отношению к
некой фильтрации $\{ \mathcal{F}_{n} \}$, то он также
супермартингал по отношению к естественной фильтрации
$\sigma(X_{0},X_{1},\ldots X_{n})$. \par
Докажите, что $X$ - супермартингал $\Leftrightarrow$ $(-X)$ -
субмартингал. \par
Докажите, что
$$
\left \{
\begin{array}{rcl}
X \text{- субмартингал} \\
X \text{- супермартингал} \\
\end{array}
\right. \Leftrightarrow X \text{- мартингал}
$$
Упражнение 3. \par
3.1. Докажите, что если $X$ - мартингал, $\phi$ - выпуклая вниз и
$\phi(X_{n})$ - интегрируемая величина то $\phi(X)$ -
субмартингал. \par
3.2. Докажите аналогичный результат для случая, когда $X$ -
субмартингал, а $\phi$ - выпуклая вниз и неубывающая \par

Пример 1. \par
$S_{0}=0$, $\E(X_{i})=0$ /кстати, из этого, в частности, следует,
что $\E(|X_{i}|)<+\infty$ - интегрируемость/, $X_{i}$ - независимы,
$S_{n}=\sum_{i=1}^{n}
X_{i}$ \par
$S$ - мартингал по отношению к фильтрации, порождаемой последовательностью $X_{i}$. \par

Пример 2. \par
$X_{i}$ - независимы, $\E(X_{i})=0$, $Var(X_{i})=\sigma^{2}$,
$M_{n}=S_{n}^{2}-n\sigma^{2}$ \par
$M_{n}$ - мартингал по отношению к $X_{i}$ \par

Пример 3. Мартингал Леви.\par
$Y$ - произвольная интегрируемая случайная величина,
$\mathcal{F}_{n}$ - произвольная фильтрация. \par
$X_{n}=\E(Y|\mathcal{F}_{n})$ - мартингал. \par

Пример 4. \par
Возрастающая последовательность действительных чисел -
субмартингал. \par

Пример 5. \par
$X_{n}\ge 0$, $X_{i}$ - независимы, $\E(X_{n})=1$, $M_{0}=1$,
$M_{n}=X_{1}\cdot X_{2}
\cdot \ldots  \cdot X_{n}$ \par
$M_{n}$ - мартингал по отношению к $X_{n}$ \par

Пример 6. \par
$Y_{i}$ - iid, $\phi(\lambda)=\E(exp(\lambda Y_{i}))<+\infty$,
тогда $X_{i}=exp(\lambda Y_{i})/\phi(\lambda)$ подходят в условия
предыдущего примера, и имеется целый набор мартингалов (для
каждого $\lambda$):
$$
M_{n}=exp\left(\lambda \sum_{i=1}^{n}
Y_{i}\right)/\phi(\lambda)^{n}
$$ \par
Среди этого набора мартингалов может быть особо полезен мартингал,
получающийся в случае такого $\lambda_{0}$, что
$\phi(\lambda_{0})=1$. \par

Пример 7. \par
Пуассоновский процесс. \par
P1. При $0=t_{0}<t_{1}<t_{2}<\ldots <t_{n}$ величины
$N(t_{k})-N(t_{k-1})$ являются независимыми. \par
P2. Величина $N(t)-N(s)$ имеет распределение Пуассона с параметром
$\lambda \cdot (t-s)$ \par

Распределение Пуассона с параметром $\lambda$:
$\P(X=n)=e^{\lambda}\frac{\lambda^{n}}{n!}$ \par

Пусть $t_{i}$ - фиксированные моменты времени и
$X_{i}=N(t_{i})-\lambda\cdot t_{i}$. \par
Тогда $(X_{i},\mathcal{F}_{i}=\sigma(N(t),t\le t_{i}))$ -
мартингал. \par


Упражнение. \par
Руководствуясь примером 6 постройте мартингал из iid
$ Y_{i}=
\left\{
\begin{array}{rcl}
1, p \\
-1, 1-p \\
\end{array}
\right.
 $ \par
При подборе $\lambda$ заметьте, что уравнение $\phi(\lambda)=1$
будет иметь два корня!

Упражнение. \par
Постройте «мультипликативный» мартингал из $ Y_{i}= \left\{
\begin{array}{rcl}
1, p \\
0, 1-p \\
\end{array}
\right. $ \par

Упражнение. \par
Докажите, что максимум двух субмартингалов (по отношению к одной
фильтрации) - также субмартингал \par

Игрок может не просто играть в честную игру, но и делать ставки.
Естественно ставка должна определяться только из прошлой
информации.

Определение. Процесс $C$ на $(\Omega,\mathcal{F},\{\mathcal{F}_{n}
\}, \mathbf{P})$ называется предсказуемым (predictable,
nonanticipating) если $C_{n} \in \mathcal{F}_{n-1}$ \par

$C_{n}$ - это размер ставки.

Игра со ставками будет формально выглядеть как:

Определение. Мартингальным преобразованием (martingale transform)
процесса $X$ называется процесс:
$$
(C\cdot X)_{n}=\sum_{i=1}^{n} C_{i} (X_{i}-X_{i-1}), $$ $$ (C\cdot
X)_{0}=0
$$

Нетрудно заметить, что $X_{i}-X_{i-1}$ - это выигрыш в $i$-ой
игре, а $C_{i}$ - ставка, т.е. множитель для выигрыша. \par

Мартингальное преобразование - это дискретная версия интеграла. \par

Теорема. Если $X$ - супермартингал, $C_{n}\ge 0$ - предсказуемый
процесс и
каждая $C_{i}$ ограничена (т.е. $\forall i \exists a : |C_{i}|<a$), то $C\cdot X$ - также супермартингал. \par
Доказательство. \par
В силу ограниченности всех $C_{i}$, следует интегрируемость $(C
\cdot X)_{n}$. \par
$$
\E((C\cdot X)_{n+1}|\mathcal{F}_{n})=(C\cdot
X)_{n}+\E(C_{n+1}(X_{n+1}-X_{n})|\mathcal{F}_{n})=
$$
$$ =(C\cdot
X)_{n}+C_{n+1}\E(X_{n+1}-X_{n}|\mathcal{F}_{n})\le (C\cdot X)_{n}
$$

Упражнение. \par
Убедитесь в том, что эта теорема верна для субмартингалов без
изменений, а для мартингалов верна даже без условия $C_{n} \ge 0$.
\par

Мораль. В честной игре бесполезно изменять ставку в зависимости от
текущего везения-невезения: это не изменит ожидаемого выигрыша.
Нельзя победить систему. "System theorem". \par
С другой стороны, честную или благоприятную игру нельзя испортить
меняя ставки даже если очень стараться :) \par
Нетрудно догадаться, что и стратегия выхода из игры после крупного
выигрыша не повлияет на ожидаемый выигрыш. Выход из игры после
крупного выигрыша равносилен нулевой ставке.  \par



Хотя с помощью игровых стратегий невозможно сделать деньги, они
пригодятся для доказательства теорем :) \par
А можно ли сделать деньги с помощью теорем? \par

Определим остановленный процесс. Остановленный процесс $X_{N\wedge
n}$ совпадает с исходным процессом $X_{n}$ до момента $N$ и равен
$X_{N}$ после. \par

Теорема. \par
Пусть $N$ - момент остановки и $X$ - супермартингал. Тогда
остановленный процесс $X_{N\wedge n}$ является супермартингалом. \par

Доказательство \par
Определим $C_{n}=1_{N\ge n}$. Стратегия «ставим по рублю вплоть до
остановки». Заметим, что процесс $C$ является предсказуемым, т.к.
$\{N\ge n\}=\{N\le n-1\}^{c}\in \mathcal{F}_{n-1}$. Из теоремы о
мартингальном преобразовании следует, что $(C\cdot
X)_{n}=X_{N\wedge n}-X_{0}$ - также является
супермартингалом. \par

Упражнение. Сформулируйте аналогичную теорему для субмартингалов и
мартингалов. \par

Пусть $X_{n}$ - субмартингал. Например, можно считать, что это
цена какой-нибудь акции. Построим график одной из
реализаций $X_{n}$. \par
Медитируем, смотрим на отсутствующий график. \par
Отметим два уровня цен $a<b$. Вопрос, на который мы сейчас в
некотором смысле ответим, звучит так: Сколько раз к моменту
времени $n$ мартингал пересечет диапазон $(a;b)$ снизу-вверх? \par
На графике таких пересечений снизу-вверх (upcrossing) \ldots  штуки. \par

Введем последовательность случайных величин $N_{i}$. Так, чтобы
$N$ с нечетными номерами означали время начала пересечения, а $N$
с четными номерами - время окончания пересечения. \par
Формально:
$$
N_{2k-1}=\inf \{m>N_{2k-2}|X_{m}\le a\} $$
$$ N_{2k}=\inf
\{m>N_{2k-1}|X_{m}\ge b\} $$
$$
N_{0}=-1
$$
Заметим, что все $N_{i}$ являются моментами остановки (о том, что
произошло событие $N_{i}=k$, можно сделать зная $X_{1}$, \ldots ,
$X_{k}$). \par
Рассмотрим событие $\{$момент времени $m$ приходится на $k$-ое
пересечение$\}$ \par
$\{N_{2k-1}<m\le N_{2k}\}=\{N_{2k-1}\le m-1 \}\cap\{N_{2k}\le
m-1\}^{c} \in \mathcal{F}_{m-1}$ \par
Рассмотрим $H_{m}= \left\{
\begin{array}{rcl}
1, \text{если} N_{2k-1}<m\le N_{2k} \text{для какого-нибудь} k \\
0, \text{иначе}
\end{array}
\right. $ \par
$H_{m}$ - предсказуемый процесс. \par
$U_{n}=\sup \{k|N_{2k}\le n\}$ - число законченных пересечений
снизу-вверх к моменту $n$. \par
Теорема. The upcrossing inequality (Doob) \par
$(b-a)\E(U_{n})\le \E(X_{n}-a)^{+}-\E(X_{0}-a)^{+}$ \par
Доказательство. \par
Вместо процесса $X_{n}$ рассмотрим процесс $Y_{n}$, который
совпадает с $X_{n}$, когда $X_{n}>a$, и равен $a$, если $X_{n}\le
a$. \par
Заметим, что $Y_{n}=a+(X_{n}-a)^{+}$, поэтому $Y_{n}$ -
субмартингал. Количество пересечений снизу вверх у $X_{n}$ и $Y_{n}$ одинаково. \par
Будем делать ставки на $Y$ согласно игровой стратегии $H$. Т.е.
будем каждый раз во время подъема от $a$ к $b$ делать ставку в $1$
рубль. \par
Каждый полный подъем от $a$ к $b$ дает выигрыш $\ge (b-a)$,
поэтому $(b-a)U_{n}\le (H\cdot Y)_{n}$. \par
В силу того, что мы делаем ставки на процесс $Y$, тот факт, что
последний подъем может быть неполным, а процесс $X$ может уйти
далеко вниз, нас нисколько не смущает. \par
Обозначим $K_{m}=1-H_{m}$. В силу теоремы о мартингальном
преобразовании, $(K\cdot Y)$
- субмартингал, поэтому, $\E(K\cdot Y)_{n} \ge \E(K\cdot Y)_{0}=0$.\par
Заметим, что $Y_{n}-Y_{0}=(H\cdot Y)_{n}+(K\cdot Y)_{n}$ (на
сколько раз рублей изменится благосостояние игрока, если каждый
раз делать ставку в $1$ рубль?). \par
Итого:
$$
(b-a)\E(U_{n})\le \E(H\cdot Y)_{n} = \E(Y_{n}-Y_{0})-\E(K\cdot Y)_{n}
\le \E(Y_{n})-\E(Y_{0})=$$ $$ =\E(X_{n}-a)^{+}-\E(X_{0}-a)^{+}
$$

В доказательстве использовался тот факт, что самая
«глупая» игровая стратегия $K$ приносит прибыль. :) \par



Подглава. Нужна ли? Равномерная интегрируемость. \par
Определение. Набор случайных величин $\{X_{\alpha}|\alpha \in A\}$
(необязательно счетный) называется равномерно интегрируемым, если:
$$
\lim_{M\rightarrow +\infty}\sup_{\alpha \in A}
E\left(|X_{\alpha}|1_{|X_{\alpha}|>M}\right)=0
$$

Пример 1. Конечный набор интегрируемых случайных величин
равномерно интегрируем. По DCT. \par

Пример 2. Если $|X_{\alpha}|<Y$ и $\E(Y)<+\infty$, то набор
$\{X_{\alpha}\}$ равномерно интегрируем. \par

Пример 3. Если набор случайных величин ограничен в смысле $L_{p}$,
$p>1$, то он равномерно интегрируем. \par
Ограничен в смысле $L_{p}$:
$sup_{\alpha}E\left(|X_{\alpha}|^{p}\right)<+\infty$. \par

Упражнение 1. \par
Равномерно интегрируемый набор ограничен в $L_{1}$.

Упражнение 2. \par
Сходящаяся в $L_{1}$ последовательность является равномерно
интегрируемой. \par

Упражнение 3. \par
Пусть $\{X_{\alpha}\}$ и $\{Y_{\beta}\}$ равномерно интегрируемы.
Тогда равномерно интегрируемо их объединение. \par

Определение. Случайная величина $X$ называется равномерно
интегрируемой, если для любой последовательности событий $A_{i}$,
такой что $\P(A_{i})\rightarrow 0$, выполнено условие $\E(X\cdot
1_{A_{i}})\rightarrow 0$. \par

Утверждение. Из равномерной интегрируемости набора
$\{X_{\alpha}\}$ следует: \par
$$
\text{UI1. } \sup_{\alpha} \E(|X_{\alpha}|)<+\infty
$$
$$
\text{UI2. } \lim_{M\rightarrow +\infty} \sup_{\alpha}
\P(|X_{\alpha}|>M)=0
$$
$$
\text{UI3. Для любой } A_{i} \text{ такой что }
\P(A_{i})\rightarrow 0 \text{ следует } \lim_{i\rightarrow +\infty}
\sup_{\alpha} \E(|X_{\alpha}|\cdot 1_{A_{i}})=0
$$
В частности из UI3 следует равномерная интегрируемость каждой
$X_{\alpha}$ \par
Утверждение. \par
Если выполнены свойства UI1 и UI3, то набор $\{X_{\alpha}\}$
равномерно интегрируем. \par
Если выполнены свойства UI2 и UI3, то набор $\{X_{\alpha}\}$
равномерно интегрируем. \par


Теорема. \par
Если $X_{n} \rightarrow X$ по вероятности, то утверждения UE1, UE2
и UE3 эквивалентны \par

Утверждение. \par
Если $X \in L_{1}$, то набор случайных величин
$\{\E(X|\mathcal{Y})|\mathcal{Y}\subseteq \mathcal{F}\}$ является
равномерно интегрируемым. \par


% про теорему о моменте остановки
% сначала в дискретном времени, потом в непрерывном

% источники:
% Ross, Second course in probability
% Stirzaker, Probability and random processes
% Williams, Probability with martingales
% Morters, Martingales
% Chang, Stochastic processes
% Blom, Problems and snapshots from probability theory


Если не ждать «слишком» долго, то ожидаемый выигрыш будет равен начальной сумме, $\E(X_{T})=\E(X_{1})$. Вот пример «слишком» долгого ожидания: ждать до выигрыша в один рубль в классическом случайном блуждании. Если $T$ - первый момент достижения суммы в один рубль, то $\E(X_{T})=X_{T}=1\neq X_{0}=0$ по построению, но ждать счастливого момента действительно придется долго: $\E(T)=+\infty$.

Точный смысл понятия «слишком» долго можно увидеть в теореме:

Если $X_{t}$ - мартингал, $T$ - момент остановки и выполнено хотя бы одно из пяти условий:

(i) Момент $T$ ограничен, то есть существует число $M$, такое что $T<M$.

(ii) $\P(T<+\infty)=1$ и процесс $X_{t\cap T}$ ограничен, то есть существует число $M$, такое что для любого $t$ верно неравенство $|X_{t\cap T}|<M$.

(iii) $\E(T)<+\infty$ и существует число $M$, такое что для любого $t$ верно неравенство $\E(|X_{t+1}-X_{t}||\mathcal{F}_{n})<M$.

(iv) $\P(T<+\infty)=1$, $\E(|X_{T}|)<\infty$ и $\lim_{t\to\infty}\E(X_{t}1_{T>t})=0$.

(v) $\P(T<+\infty)=1$ и мартингал $X_{t}$ является равномерно интегрируемым.

В большинстве случаев первых трех критериев достаточно для практического применения.
Четвертый критерий является следствием любого из первых трех (?).

В пятом критерии используется определение равномерной интегрируемости\ldots
Набор случайных величин является равномерно интегрируемым, если\ldots


Из этих критериев все кроме третьего (может есть какой-то «предельный» аналог и третьего? - я не знаю) работают в непрерывном времени.


Примеры:

% включить доказательство без теоремы Дуба

Тождество Вольда. Wald's identity.

Симметричное случайное блуждание, $T=min\{t|S_{t}=a\cup S_{t}=-b\}$ - подсчет $\P(S_{T}=a)$ (через $S_{t}$) и $\E(T)$ (через $S_{t}^{2}-t$)

Несимметричное случайное блуждание - подсчет $\P(S_{T}=a)$ (через $(p/q)^{S_{t}}$)

ABRACADABRA - $\E(T)$, $Var(T)$ - см. Ross, Second course

Следующая карта - красная (через долю красных карт)

Раунды при подбрасывании шляп (через $\sum X_{i} -n$) % Ross, Second

Ключи и сейфы

Ballot problem % Ross, Second

Second heart problem % Morters, Martingales, p. 31






Задачи:

% задачи из Stirzaker, 12.5

$\E(TS_{T})$ для классического случайного блуждания - ?


% идеология: лучше сделать лишний повтор, ибо: 1 - можно рассказать несколько раз, не опасаясь, что кто-нибудь поймет :), 2 - появляется большая независимость глав.



Задача.

Пусть $Y_{k}$ - количество посещений точки 0 симметричным случайным блужданием до посещения точки $k$. Заметим, что случайное блуждание может никогда не посетить точку $k$, но это событие происходит с вероятностью 0, поэтому величину $Y_{k}$ можно считать корректно определенной.
Верно ли, что $Y_{k}-2k$ - мартингал?

Решение:

\Closesolutionfile{solution_file}
